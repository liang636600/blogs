# 运行`GroupByRDD-sample.sh`

## 解析`GroupByRDD-sample.sh`

![image-20211127101248921](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20211127101248921.png)

### 1 输入

`UserVisits.txt`文件作为输入，该文件内容为

![image-20211127101654884](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20211127101654884.png)

其中一行的数据为

`169.124.27.16|2jmsklgffzpkfsvjfgreppkhwwlsfxwygjrrvxxbwmlcfgmlmdqctakpwvictfazpsrkqp.html|1983-11-23|484.530311958|Mozilla/2.0 compatible; Check&Get 1.1x|GMB|GMB-FS|processing|2`总共有9列，共1000条数据

一行数据解释是`sourceIP destURL visitDate adRevenue userAgent countryCode languageCode searchWord duration`

### 2 输出

输入在一个文件夹下

![image-20211127102310015](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20211127102310015.png)

这里共有10个part（编号0~9），应该意味着10个partition，每个partition对应一个part

打开一个part文件

![image-20211127102509668](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20211127102509668.png)

其中文件的一行数据为

```
(165.113,CompactBuffer([165.113.15.40,4tyljkwmommzklsnboqjpzoic.html,1994-7-27,325.16875438,ozelot/2.7.3,PER,PER-XH,black,7], [165.113.25.28,5ecaznwfftso.html,1979-7-13,363.249884691,ZipppBot/0.xx,ARM,ARM-AQ,UV,5]))
```

结合该函数应该是GroupBy，所以该脚本的功能是将IP前两位作为key，groupby

### logFile

程序运行期间的log放在logFile中

### 运行的Scala文件

运行的是`applications.sql.rdd.RDDGroupByTest`这个文件

在这个文件中加上注释理解

```scala
package applications.sql.rdd

import org.apache.spark.sql.{Row, SparkSession}
import org.apache.spark.sql.types.{StringType, StructField, StructType}

/**
  * Created by xulijie on 17-6-21.
  *
  * SELECT * FROM UserVisits GROUP BY SUBSTR(sourceIP, 1, 7);
  */
object RDDGroupByTest {
  def main(args: Array[String]): Unit = {

    if (args.length < 2) {
      System.err.println("Usage: RDDGroupByTest <table_hdfs_file> <output_file>")
      System.exit(1)
    }

    // $example on:init_session$
    val spark = SparkSession
      .builder()
      .getOrCreate()


    // $example off:init_session$
    // $example on:programmatic_schema$
    // Create an RDD
	// args(0)对应的输入文件路径
    val uservisits = spark.sparkContext.textFile(args(0))


    // The schema is encoded in a string
    val uservisitsSchemaString = "sourceIP destURL visitDate adRevenue userAgent countryCode languageCode searchWord duration"

    // Generate the schema based on the string of schema
    // StructField(String name, DataType dataType, boolean nullable)
    val uservisitsFields = uservisitsSchemaString.split(" ")
      .map(fieldName => StructField(fieldName, StringType, nullable = true))
    val uservisitsSchema = StructType(uservisitsFields)

    // Convert records of the RDD (people) to Rows
    val uservisitsRDD = uservisits
      .map(_.split("\\|"))
      .map(attributes => (attributes(0).substring(0, 7), Row(attributes(0), attributes(1), attributes(2), attributes(3), attributes(4), attributes(5),
        attributes(6), attributes(7), attributes(8))))

    // Apply the schema to the RDD
    // val uservisitsDF = spark.createDataFrame(uservisitsRDD, uservisitsSchema)

    // Creates a temporary view using the DataFrame
    // uservisitsDF.createOrReplaceTempView("uservisits")

    // SQL can be run over a temporary view created using DataFrames
    // val results = spark.sql("SELECT name FROM people")

    val result = uservisitsRDD.groupByKey()
	// args(1)表示输出路径，最后为一个textFile
    result.saveAsTextFile(args(1))
  }

}
```

# 尝试运行GroupBy的完整数据


# 1 

![image-20211120144320077](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20211120144320077.png)

2>&1的解释：

0 stdin，1 stdout，2 stderr

2>&1应该分成两个部分来看，一个是2>以及另一个是&1，其中2>就是将标准出错重定向到某个特定的地方；&1是指无论标准输出在哪里。所以2>&1的意思就是说无论标准出错在哪里，都将标准出错重定向到标准输出中

tee从标准输入中读取，并将读入的内容写到标准输出以及文件中，这里$7应该是log文件

# 2

doCommand.sh文件是Linux批量创建主机信任关系

# 3 尝试运行一个sample

## 3.1 首先打包源码为一个jar包

使用sbt打包

---


报错`object SparkSession is not a member of package org.apache.spark.sql`

* 尝试修改simple.sbt在末尾加上一行`libraryDependencies += "org.apache.spark" %% "spark-sql" % "3.2.0"`

---

报错`LogisticRegressionWithSGDTest.scala:4:25: object mllib is not a member of package org.apache.spark`

* 尝试在simple.sbt的末尾加上一行`libraryDependencies += "org.apache.spark" %% "spark-mllib" % "3.2.0"`

---

报错`LogisticRegressionWithSGDTest.scala:28:17: not found: value LogisticRegressionWithSGD`

* 尝试删除LogisticRegressionWithSGDTest文件

---

## 3.2 运行shell脚本

提示权限不够`bash: ./GroupByRDD-sample.sh: 权限不够`

* 尝试`chmod 777 ./GroupByRDD-sample.sh`再运行

---

运行报错`JAVA_HOME is not set`

* 因为这里是root在运行，所以短期解决方案是直接加上`export JAVA_HOME=/home/iscas/Downloads/openjdk-16_linux-x64_bin/jdk-16`

---

运行成功生成一个RDDGroupByTest文件夹

![image-20211120164905120](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20211120164905120.png)

# 4 尝试使用ZGC运行

修改参数为使用ZGC `"-XX:+UseZGC`

运行也成功

# 5 监控GC相关参数

jvm options在jdk11后出现了改动，以下的jvm参数改动

```
-XX:+PrintGCTimeStamps    
-XX:+PrintGCDateStamps    ==>  decoration options
                               -Xlog:::time,level,tags
-XX:+PrintGCDetails       ==>  -Xlog:gc*
-XX:+PrintGCApplicationStoppedTime ==> -Xlog:safepoint
```

因此修改`GroupByRDD-sample.sh`里的内容

```
--conf "spark.driver.extraJavaOptions=--illegal-access=permit -XX:+UseZGC  -verbose:gc -Xlog:gc:/tmp/Drivergc.log -Xlog:gc* -Xlog:::time,level,tags -Xlog:safepoint"  --conf spark.executor.extraJavaOptions="--illegal-access=permit -XX:+UseZGC  -verbose:gc -Xlog:gc:/tmp/executorgc.log -Xlog:gc* -Xlog:::time,level,tags -Xlog:safepoint" 
```

发现在日志中确实是使用的ZGC，并且打印了GC日志

![image-20211127130529781](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20211127130529781.png)

## web UI相关

每一个SparkContext都会有一个Web UI，默认在4040端口(http://<driver-node>:4040)

想通过WebUI控制台页面来查看具体的job运行细节，正在运行时的任务可以通过访问 **http:ip/4040** (一旦任务结束，该端口将会自动关闭)，有时需要查看历史的job，这时需要用到 **spark-history-server.sh** 服务，操作如下：

具体参考<https://blog.csdn.net/saranjiao/article/details/106239872>中的Web-UI

可以启动history-server来记录log，`./sbin/start-history-server.sh`，默认运行在http://<server-url>:18080，通过spark-history-server可以查看一些统计信息，目前的话只能查看到GC time和duration

![image-20211127175329904](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20211127175329904.png)

## GC参数监测

在<https://github.com/JerryLead/SparkProfiler>中，作者设置了三种profiler可以统计运行时间

* The execution time profiler measures the execution time of each application and each map/reduce task
* The dataflow profiler collects the number and size of the records in each data processing phase. We extend the Spark log system to record the spilled data size and spill time
* The resource profiler collects the CPU, memory usage, and GC metrics of each task

### 把程序运行时的gc信息保存为文件

在conf参数中加上

```
-Xlog:gc:/home/iscas/Desktop/Drivergc.log
```

gc:后面是log的路径

![image-20211127183751619](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20211127183751619.png)

### 尝试运行一次完整数据

#### 生成完整数据

(1) 安装anaconda

<https://www.anaconda.com/products/individual#linux>

`bash ~/Downloads/Anaconda3-2021.11-Linux-x86_64.sh`

(2) 运行./generateData.py

报错`bash: ./generateData.py: /usr/bin/python: bad interpreter: No such file or directory`

* 尝试删除`generateData.py`第一行内容，用`python generateData.py `

---

报错

```
File "/home/iscas/Desktop/SparkGC/datagen/sql/htmlgen/generateData.py", line 13
    print "\nUsage: %s <config>\n" % sys.argv[0]   
          ^
SyntaxError: Missing parentheses in call to 'print'. Did you mean print("\nUsage: %s <config>\n" % sys.argv[0])?
```

* 尝试将源文件中所有的print语句后加上()

---

报错

```
Traceback (most recent call last):
  File "/home/iscas/Desktop/SparkGC/datagen/sql/htmlgen/generateData.py", line 6, in <module>
    import sys, os, thread, commands
ModuleNotFoundError: No module named 'thread'
```

* 尝试将thread改成_thread

---

报错

```
Traceback (most recent call last):
  File "/home/iscas/Desktop/SparkGC/datagen/sql/htmlgen/generateData.py", line 6, in <module>
    import sys, os, _thread, commands
ModuleNotFoundError: No module named 'commands'
```

把commands改为subprocess

同时将`generateData.py`文件里的内容涉及到thread的地方修改为_thread，涉及到commands的地方修改为subprocess

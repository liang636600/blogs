**目标：**求解函数的极小值

**方法：**梯度方向是目标函数值增加最快的方向，梯度负方向函数值下降最快，因此将自变量沿着梯度负方向变化，函数值一般会减少

# 1 Batch梯度下降

这里以线性回归算法为例，Batch梯度下降指的是：**遍历整个训练集**计算平均梯度值，更新一次参数值θ（梯度值如下面蓝色方框内容所示）

![image-20220603161020027](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20220603161020027.png)

## 缺点

当训练的数据集较大的时候（比如上亿条数据），计算一次迭代的梯度值需要遍历整个数据集导致训练较慢

# 2 随机梯度下降（SGD）

**优点：**可以更好处理大型数据集

**核心：**每一条训练数据流进来，更新一次参数值

## 步骤

1. 随机打乱训练集
2. 对训练集中的每一条数据流进来的时候，更新一次θ的值

![image-20220603165724805](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20220603165724805.png)

**效果**：整个过程以随机迂回的方式朝全局最小值前进，如下图粉红色曲线所示，而红色曲线表示的是Batch梯度下降

![image-20220603165958263](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20220603165958263.png)

repeat的次数一般是1~10次

## 如何保证收敛

每1000个迭代时，算出前1000个样本的cost（cost是指一条新的样本流入时，使用之前的模型算一下cost值）的平均值，把这个平均值画出来（平均值的趋势应该是递减的）

![image-20220603172155876](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20220603172155876.png)

画出来的平均值的图可能出现以下的四种情况

* 图一中蓝色线表示每1000个迭代（学习算法收敛），红线表示使用了一个更小的学习速率α
* 图二中蓝线表示每1000个迭代，红线表示每5000个迭代，红线看起来更加平滑，但也存来延迟的问题
* 图三中蓝线表示每1000个迭代（噪声较大），红线表示每5000迭代，粉红线表示每5000迭代（但算法不知什么原因出现没有学习的情况，可以采用调整学习速率或调整特征的方式）
* 图四表示每1000个迭代，cost的平均值上升，这种情况表示发散不收敛，可以采取的措施是使用更小的学习率α

![image-20220603174654820](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20220603174654820.png)

# 3 Mini-Batch梯度下降

 **核心：**每次迭代更新参数会使用b个样本，一般b的值为2~100

![image-20220603171204364](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20220603171204364.png)

**特点：**介于Batch梯度下降算法与随机梯度下降算法之间

# 4 最速下降法

学习率α随着迭代而变化

![image-20220604182614877](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20220604182614877.png)

![image-20220604182856585](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20220604182856585.png)

确定α（下图中为tk）的方法

![img](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/v2-f980211e9143f54a40c05e000c78d7aa_720w.jpg)

![img](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/v2-e39fc7952c26563af3c888667e5af8d8_720w.jpg)

代码实现

![image-20220608123212393](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20220608123212393.png)

迭代次数比较：固定步长的梯度下降法>最速梯度下降法>**BB算法**

固定步长的梯度下降法的代码（f表示原函数，g表示梯度）

![image-20220608123652606](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20220608123652606.png)

![image-20220608123809941](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20220608123809941.png)

最速梯度下降法的代码

![image-20220608124150114](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20220608124150114.png)

搜索α的最优值（f表示原函数，g表示梯度函数，d的值为-gk），下面代码中φ0其实是（φ0‘），下面代码中Armijo condition对应代码第二行的注释（表示α不太大条件，如果不满足这个条件，表示α比较大，所以缩小α的值）；下面代码中Wolfe condition对应代码注释第四行（表示α不太小条件，如果不满足这个条件，表示α比较小，应该放大α的值）

fn表示fk+1，gn表示gk+1

![1654675623450](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1654675623450.jpg)

![image-20220608124233195](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20220608124233195.png)

BB算法（更新α的方法如下面公式）代码

## ![image-20220608122816681](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20220608122816681.png)

下面代码中sk的值为δ

![image-20220608125942098](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20220608125942098.png)

## 二次型函数最速下降法

![image-20220604190225914](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20220604190225914.png)

![image-20220604190654108](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20220604190654108.png)


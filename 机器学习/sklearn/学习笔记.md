# 安装

```
pip install scikit-learn
```

![scikit-learn算法图表](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/ml_map.png)

sklearn下载数据集在本地位置：D:\Anaconda\Lib\site-packages\sklearn\datasets\data

# 通用代码模式

```python
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

iris = datasets.load_iris()
# 可以在load_iris中使用参数return_X_y=True
# X, y = load_iris(return_X_y=True)
iris_X = iris.data
iris_y = iris.target
X_train,X_test,y_train, y_test = train_test_split(iris_X, iris_y, test_size=0.3) # 测试的比例占到了30%的总数据，分开了并打乱了数据，test_size也可以为整数

knn = KNeighborsClassifier()
# 训练模型
knn.fit(X_train,y_train)
# 对测试集中的数据预测
print(knn.predict(X_test))
print(y_test)
```

# sklearn的datasets

![img](https://pic1.zhimg.com/80/v2-1a20768997bbe8cdf11449ed27954fb4_720w.jpg)

![img](https://pic3.zhimg.com/80/v2-02082a84f01003011bfd0abcc3ad47ba_720w.jpg)

## 使用自带的datasets

```python
from sklearn import datasets
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

loaded_data = datasets.load_boston()
data_X = loaded_data.data
data_y = loaded_data.target

model = LinearRegression()
model.fit(data_X,data_y)
print(model.predict(data_X[:4,:]))
print(data_y[:4])
```

## 使用生成的datasets

### make_regression

```python
from sklearn import datasets
import matplotlib.pyplot as plt

X,y = datasets.make_regression(n_samples=100, n_features=1, n_targets=1,noise=1,random_state=20)
plt.scatter(X,y)
plt.show()
```

### make_classification

```python
from sklearn import datasets
import matplotlib.pyplot as plt

X, y = datasets.make_classification(n_samples=100, n_features=2, n_informative=1,
                                    n_redundant=0, n_classes=2, n_clusters_per_class=1, scale=1.0,
                                    random_state=20)

plt.scatter(X[:, 0], X[:, 1], c=y)
plt.show()
```

### make_blobs（聚类）

```python
from sklearn.datasets import make_blobs
from matplotlib import pyplot

data,label = make_blobs(n_samples=100,n_features=2,centers=5)
# 绘制样本显示
pyplot.scatter(data[:,0],data[:,1],c=label)
pyplot.show()
```

# Model常用属性和功能

```Python
from sklearn import datasets
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

loaded_data = datasets.load_boston()
data_X = loaded_data.data
data_y = loaded_data.target

model = LinearRegression()
model.fit(data_X,data_y)
print(model.predict(data_X[:4,:]))
print(data_y[:4])
# 获得模型参数
print(model.coef_) # 例如y=0.1x+0.3这里输出0.1
print(model.intercept_) # 这里输出0.3
print(model.get_params()) # 返回模型定义的参数{'copy_X': True, 'fit_intercept': True, 'n_jobs': None, 'normalize': False}
print(model.score(data_X,data_y)) # 对学到的模型进行打分，线性回归这里使用的R^2 coefficient of determination，分类问题使用acc
```

# 数据预处理

## 标准化数据（normalization）

标准化变换后各维特征有0均值，单位方差，也叫z-score规范化（零均值规范化），计算方式是将特征值减去均值，除以标准差

```PYTHON
from sklearn import preprocessing
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
from sklearn.svm import SVC
import matplotlib.pyplot as plt
# a = np.array([[10,2.7,3.6],[-100,5,-2],[120,20,40]],dtype=np.float64)
# print(preprocessing.scale(a))
X,y = make_classification(n_samples=300,n_features=2,n_redundant=0,n_informative=2,random_state=22,n_clusters_per_class=1,scale=1000) # random_state表示随机产生，每次产生的data一样
# plt.scatter(X[:,0],X[:,1],c=y)
# plt.show()
# 标准化
# X = preprocessing.minmax_scale(X,feature_range=(0,1)) # X标准化后的范围，默认从0到1
X = preprocessing.scale(X)
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)
clf=SVC()
clf.fit(X_train,y_train)
print(clf.score(X_test,y_test))
```

# 交叉验证

![image-20220612162622529](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20220612162622529.png)

```Python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

iris = load_iris()
X = iris.data
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4)

from sklearn.model_selection import cross_val_score
knn = KNeighborsClassifier(n_neighbors=5)
scores = cross_val_score(knn,X,y,cv=5,scoring='accuracy') # cv表示把源数据分成了几份，取其中一份作为测试集; scoring='accuracy'用作分类问题scoring='neg_mean_squared_error'用作回归问题，用作回归时，cross_val_score前面加上负号
print(scores)
print(scores.mean())

import matplotlib.pyplot as plt
k_range = range(1,31)
k_scores = []
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn,X,y,cv=10,scoring='accuracy')
    k_scores.append(scores.mean())

plt.plot(k_range,k_scores)
plt.xlabel('Value of K for KNN')
plt.ylabel('Cross-Validated Accuracy')
plt.show()
```

## 可视化训练集和测试集的学习过程（loss图，横坐标训练集的数量）

```python
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.model_selection import learning_curve
from sklearn.svm import SVC
import matplotlib.pyplot as plt
import numpy as np
digits = load_digits()
X = digits.data
y = digits.target
# 下面的train_sizes对应在训练集经历数据为多少（个）的时候记录scoring的值，例如总数据1797条，cv=10即10折交叉验证，则训练集数据共1797*0.9条，train_sizes的值为[1797*0.9*0.1,1797*0.9*0.25...]
# train_loss的数据维度为(size(train_sizes), cv的值)
train_sizes, train_loss,test_loss=learning_curve(SVC(gamma=0.001),X,y,cv=10,scoring='neg_mean_squared_error',train_sizes=[0.1,0.25,0.5,0.75,1]) # train_sizes表示训练集每经历[0.1,0.25,0.5,0.75,1]数据记录下来scoring的值
train_loss_mean = -np.mean(train_loss,axis=1) # 10个值的平均，axis=1表示对每一行平均
test_loss_mean = -np.mean(test_loss,axis=1)
plt.plot(train_sizes,train_loss_mean,'o-',color='r',label="Training")
plt.plot(train_sizes,test_loss_mean,'o-',color='g',label="Cross-validation")
plt.xlabel("Training examples")
plt.ylabel("Loss")
plt.legend(loc="best")
plt.show()
```

![image-20220612174219605](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20220612174219605.png)

## 探究不同参数对loss的影响

```python
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.model_selection import validation_curve
from sklearn.svm import SVC
import matplotlib.pyplot as plt
import numpy as np
digits = load_digits()
X = digits.data
y = digits.target
param_range=np.logspace(-6,-2.3,6) # 在范围（-6，-2.3）中取6个点
# train_loss的数据维度为(size(param_range), cv的值)
train_loss,test_loss=validation_curve(SVC(),X,y,param_name='gamma',param_range=param_range,cv=10,scoring='neg_mean_squared_error')
train_loss_mean = -np.mean(train_loss,axis=1) # 10个值的平均，axis=1表示对每一行平均
test_loss_mean = -np.mean(test_loss,axis=1)
plt.plot(train_loss_mean,'o-',color='r',label="Training")
plt.plot(test_loss_mean,'o-',color='g',label="Cross-validation")
plt.xlabel("gamma")
plt.ylabel("Loss")
plt.legend(loc="best")
plt.show()
```

![image-20220612174611399](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/image-20220612174611399.png)

# 保存模型

```python
from sklearn import svm
from sklearn import datasets
clf = svm.SVC()
iris = datasets.load_iris()
X,y = iris.data,iris.target
clf.fit(X,y)

# 保存
# 方法一：pickle
import pickle
# 写入文件
with open('clf.pickle', 'wb') as f:
    pickle.dump(clf,f)
# 读入文件
with open('clf.pickle','rb') as f:
    clf2 = pickle.load(f)

# 方法二：joblib
# 要pip install joblib
# import joblib
# # 保存
# joblib.dump(clf,'clf.pkl')
# # 读入模型
# clf3 = joblib.load('clf.pkl')

y_predict = clf2.predict(X)
print(sum(y_predict-y))
```


# 1 DataStream的基本概念

## WordCount实例

```JAVA
import org.apache.flink.api.common.functions.FlatMapFunction;
import org.apache.flink.api.common.typeinfo.Types;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.api.common.functions.FilterFunction;
import org.apache.flink.util.Collector;

public class StreamingJob {
    public static final String[] WORDS = new String[]{
            "com.intsmaze.flink.streaming.window.helloword.WordCountTemplate",
            "com.intsmaze.flink.streaming.window.helloword.WordCountTemplate",
            "com.intsmaze.flink.streaming.window.helloword.WordCountTemplate",
            "com.intsmaze.flink.streaming.window.helloword.WordCountTemplate",
    };

    public static void main(String[] args) throws Exception {
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        DataStream<String> text = env.fromElements(WORDS);
        DataStream<Tuple2<String, Integer>> word = text.flatMap(new FlatMapFunction<String, Tuple2<String, Integer>>() {
            // 对于text中的每一条记录，使用map方法，即先把String split后，加入到集合out中去，out的结果为word变量
            @Override
            public void flatMap(String value, Collector<Tuple2<String, Integer>> out) throws Exception {
                String[] tokens = value.toLowerCase().split("\\.");
                for (String token : tokens) {
                    if (token.length() > 0) {
                        out.collect(new Tuple2<>(token, 1));
                    }
                }
            }
        });
        DataStream<Tuple2<String, Integer>> counts = word.keyBy("f0").sum(1);
        counts.print("hello dataStream");
        env.execute();
    }


}
```

## 数据源

如果Flink内置的数据源函数不满足开发者需求，通过为非并行数据源实现SourceFunction接口或为并行数据源实现ParallelSourceFunction接口或扩展RichParallelSourceFunction抽象类来编写自己的定制数据源

### 基于文件

![1654052373749](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1654052373749.jpg)

![1654052395686](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1654052395686.jpg)

![1654052703970](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1654052703970.jpg)

![1654052728656](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1654052728656.jpg)

* 实例1，直接使用readTextFile读取文件

  创建文件a.txt内容如下

  ```
  hello world
  what a good day
  there is 
  ```

  ```JAVA
  final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
          DataStreamSource<String> stringDataStreamSource = env.readTextFile("///home/driverliang/a.txt");
          stringDataStreamSource.print();
          env.execute();
  ```

* 实例2，使用readFile监控读取文件

  每隔一段时间扫描文件，当文件改变时重新读取文件所有内容处理

  ```JAVA
  final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
  
  String filePath = "///home/driverliang/files";
  TextInputFormat format = new TextInputFormat(new Path(filePath));
  TypeInformation<String> typeInfo = BasicTypeInfo.STRING_TYPE_INFO;
  DataStream<String> dataStream = env.readFile(format, filePath,
                                               FileProcessingMode.PROCESS_CONTINUOUSLY, 5000, typeInfo);
  
  dataStream.print();
  env.execute();
  ```

### 基于Socket

![1654055970940](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1654055970940.jpg)

### 基于集合

![1654056233799](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1654056233799.jpg)

### 自定义数据源函数

![1654056447642](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1654056447642.jpg)

## 数据接收器（数据保存）Sink

### 基于文件

![1654056665702](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1654056665702.jpg)

![1654056813232](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1654056813232.jpg)

### 基于标准输出流

![1654057115886](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1654057115886.jpg)

### 基于Socket

![1654057246171](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1654057246171.jpg)

### 自定义数据接收器函数

![1654057593851](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1654057593851.jpg)

# 2 数据流基本操作

## Map

```JAVA
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        DataStream<Long> streamSource = env.generateSequence(1, 5);

        DataStream<Tuple2<Long, Integer>> mapStream = streamSource
                .map(new MapFunction<Long, Tuple2<Long, Integer>>() {
                    @Override
                    public Tuple2<Long, Integer> map(Long values) {
                        return new Tuple2<>(values * 100, values.hashCode());
                    }
                });
        mapStream.print("输出结果");
        env.execute();
```

## FlatMap

数据流中的每个元素将作为输入元素进入FlatMapFunction函数，FlatMapFunction函数将对输入的元素进行转换并产生0个，1个或多个结果元素输出到新的数据流中，典型应用场景是拆分不需要的列表和数组

```java
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStream<Tuple2<String, Integer>> streamSource = env.fromElements(
                new Tuple2<>("liu yang", 1),
                new Tuple2<>("my blog is intsmaze", 2),
                new Tuple2<>("hello flink", 2));

        DataStream<Tuple1<String>> resultStream = streamSource
                .flatMap(new FlatMapFunction<Tuple2<String, Integer>, Tuple1<String>>() {
                    @Override
                    public void flatMap(Tuple2<String, Integer> value,
                                        Collector<Tuple1<String>> out) {

                        if ("liu yang".equals(value.f0)) {
                            return;
                        } else if (value.f0.indexOf("intsmaze") >= 0) {
                            for (String word : value.f0.split(" ")) {
                                out.collect(Tuple1.of("Split intsmaze：" + word));
                            }
                        } else {
                            out.collect(Tuple1.of("Not included intsmaze：" + value.f0));
                        }
                    }
                });
        resultStream.print("输出结果");

        env.execute("FlatMapTemplate");
```

## Filter

返回true表示保留该元素，典型应用场景是数据去重

```JAVA
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        DataStream<Long> streamSource = env.generateSequence(1, 5);
        DataStream<Long> filterStream = streamSource.filter(new FilterFunction<Long>() {
            @Override
            public boolean filter(Long value) throws Exception {
                if (value == 2L || value == 4L) {
                    return false;
                }
                return true;
            }
        });
        filterStream.print("输出结果");
        env.execute("Filter Template");
```

## KeyBy

具有相同key的元素print操作符的同一个子任务中进行处理

实现DataStream -> KeyedStream转换

```JAVA
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(10);
        List<Tuple2<Integer, Integer>> list = new ArrayList<Tuple2<Integer, Integer>>();
        list.add(new Tuple2<>(1, 11));
        list.add(new Tuple2<>(1, 22));
        list.add(new Tuple2<>(3, 33));
        list.add(new Tuple2<>(5, 55));

        DataStream<Tuple2<Integer, Integer>> dataStream = env.fromCollection(list);

        KeyedStream<Tuple2<Integer, Integer>, Tuple> keyedStream = dataStream.keyBy(0);

        keyedStream.print("输出结果");

        env.execute("KeyByTemplate");
```

## Reduce

实现KeyedStream -> DataStream 转换

```JAVA
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        List<Tuple3<Tuple2<Integer, String>, Integer, Integer>> list = new ArrayList<Tuple3<Tuple2<Integer, String>, Integer, Integer>>();
        list.add(new Tuple3(new Tuple2(1, "intsmaze"), 1, 11));
        list.add(new Tuple3(new Tuple2(1, "intsmaze"), 1, 22));
        list.add(new Tuple3(new Tuple2(33, "liuyang"), 33, 333));

        DataStream<Tuple3<Tuple2<Integer, String>, Integer, Integer>> dataStream = env
                .fromCollection(list)
                .keyBy("f0.f1")
                .reduce(new ReduceFunction<Tuple3<Tuple2<Integer, String>, Integer, Integer>>() {
                    @Override
                    public Tuple3<Tuple2<Integer, String>, Integer, Integer> reduce(
                            Tuple3<Tuple2<Integer, String>, Integer, Integer> value1,
                            Tuple3<Tuple2<Integer, String>, Integer, Integer> value2) {
                        Tuple2 tuple2 = value1.getField(0);
                        int v1 = value1.getField(1);
                        int v2 = value2.getField(1);
                        int v3 = value1.getField(2);
                        int v4 = value2.getField(2);
                        return new Tuple3(tuple2, v1 + v2, v3 + v4);
                    }
                });
        dataStream.print();
        env.execute("KeyByTemplate");
```

## Aggregations

![1654479400428](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1654479400428.jpg)

```java
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
List<Trade> list = new ArrayList<Trade>();
list.add(new Trade("188XXX", 30, "2018-07"));
list.add(new Trade("188XXX", 20, "2018-11"));
list.add(new Trade("158XXX", 1, "2018-07"));
list.add(new Trade("158XXX", 2, "2018-06"));
DataStream<Trade> streamSource = env.fromCollection(list);

KeyedStream<Trade, Tuple> keyedStream = streamSource.keyBy("cardNum");

keyedStream.sum("trade").print("sum");

keyedStream.min("trade").print("min");

keyedStream.minBy("trade").print("minBy");
env.execute("KeyByTemplate");
```

## Project

![1654481184179](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1654481184179.jpg)

```java
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

List<Tuple3<String, Integer, String>> list = new ArrayList<Tuple3<String, Integer, String>>();
list.add(new Tuple3("185XXX", 899, "周一"));
list.add(new Tuple3("155XXX", 1199, "周二"));
list.add(new Tuple3("138XXX", 19, "周三"));
DataStream<Tuple3<String, Integer, String>> streamSource = env.fromCollection(list);

DataStream<Tuple2<String, String>> result = streamSource.project(2, 0);
result.print("输出结果");
env.execute("Project Template");
```

![1654481490350](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1654481490350.jpg)

## Union

合并多个相同类型的数据流

```JAVA
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        DataStreamSource<Long> dataStream = env.generateSequence(1, 2);
        DataStreamSource<Long> otherStream = env.generateSequence(1001, 1002);
        DataStream<Long> union = dataStream.union(otherStream);
        union.print("result");
        env.execute("Union Template");
```

## Connect和CoMap、CoFlatMap

Connect操作符将连接两个保存其类型的数据流来创建新的连接流

数据流转换：DataStream+DataStream->ConnectedStream

```
ConnectedStreams<Long, String> connectedStreams=longStream.connect(strStream);
```

### CoMapFunction函数

将CoMapFunction函数应用于ConnectedStreams中的每个元素

```JAVA
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        List<Long> listLong = new ArrayList<Long>();
        listLong.add(1L);
        listLong.add(2L);

        List<String> listStr = new ArrayList<String>();
        listStr.add("www cnblogs com intsmaze");
        listStr.add("hello intsmaze");
        listStr.add("hello flink");
        listStr.add("hello java");

        DataStream<Long> longStream = env.fromCollection(listLong);
        DataStream<String> strStream = env.fromCollection(listStr);
        ConnectedStreams<Long, String> connectedStreams = longStream.connect(strStream);
        DataStream<String> connectedMap = connectedStreams
                .map(new CoMapFunction<Long, String, String>() {
                    @Override
                    public String map1(Long value) {
                        return "数据来自元素类型为long的流" + value;
                    }

                    @Override
                    public String map2(String value) {
                        return "数据来自元素类型为String的流" + value;
                    }
                });
        connectedMap.print();
        env.execute("CoMapFunction");
```

### CoFlatMapFunction函数

```JAVA
ConnectedStreams<Long, String> connectedStreams = longStream.connect(strStream);
        DataStream<String> connectedFlatMap = connectedStreams
                .flatMap(new CoFlatMapFunction<Long, String, String>() {
                    @Override
                    public void flatMap1(Long value, Collector<String> out) {
                        out.collect(value.toString());
                    }

                    @Override
                    public void flatMap2(String value, Collector<String> out) {
                        for (String word : value.split(" ")) {
                            out.collect(word);
                        }
                    }
                });
```

# 3 富函数

![1654483754279](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1654483754279.jpg)

```JAVA
import org.apache.flink.api.common.functions.RichFlatMapFunction;
import org.apache.flink.api.common.functions.RuntimeContext;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.util.Collector;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class RichFunctionTemplate extends RichFlatMapFunction<Long, Long> {

    public static Logger LOGGER = LoggerFactory.getLogger(RichFunctionTemplate.class);

    public static void main(String[] args) throws Exception {

        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(2);

        DataStream<Long> streamSource = env.generateSequence(1, 100);
        DataStream<Long> dataStream = streamSource
                .flatMap(new RichFunctionTemplate())
                .name("intsmaze-flatMap");
        dataStream.print();

        env.execute("RichFunctionTemplate");
    }

    @Override
    public void open(Configuration parameters) {
        RuntimeContext rc = getRuntimeContext();
        String taskName = rc.getTaskName();
        String subtaskName = rc.getTaskNameWithSubtasks();
        int subtaskIndexOf = rc.getIndexOfThisSubtask();
        int parallel = rc.getNumberOfParallelSubtasks();
        int attemptNum = rc.getAttemptNumber();
        LOGGER.info("调用open方法,初始化资源信息..");
        LOGGER.info("调用open方法,任务名称:{}...带有子任务的任务名称：{}..并行子任务的标识：{}..当前任务的总并行度:{}", taskName, subtaskName, subtaskIndexOf, parallel);
        LOGGER.info("调用open方法,该任务因为失败进行重启的次数:{}", attemptNum);

    }

    @Override
    public void flatMap(Long input, Collector<Long> out) throws Exception {
        Thread.sleep(1000);
        out.collect(input);
    }

    @Override
    public void close() {
        System.out.println("调用close方法 ----------------------->>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>");
        LOGGER.info("调用close方法 -----------------------");
    }
}
```

# 4 任务链和资源组

![1654485264269](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1654485264269.jpg)

## 默认链接

自定义数据源

```JAVA
public class ChainSource extends RichSourceFunction<Tuple2<String, Integer>> {
    private static final long serialVersionUID = 1L;

    int sleep = 3000;

    @Override
    public void run(SourceContext<Tuple2<String, Integer>> ctx) throws Exception {
        String subtaskName = getRuntimeContext().getTaskNameWithSubtasks();
        String info = "source操作所属子任务名称:";
        Tuple2 tuple2 = new Tuple2("185XXX", 899);
        ctx.collect(tuple2);
        System.out.println(info + subtaskName + ",元素:" + tuple2);
        Thread.sleep(sleep);
        //向数据流中发送元素
        tuple2 = new Tuple2("155XXX", 1199);
        ctx.collect(tuple2);
        System.out.println(info + subtaskName + ",元素:" + tuple2);
        Thread.sleep(sleep);
        //向数据流中发送元素
        tuple2 = new Tuple2("138XXX", 19);
        ctx.collect(tuple2);
        System.out.println(info + subtaskName + ",元素:" + tuple2);
        Thread.sleep(sleep);
    }

    @Override
    public void cancel() {
    }

}
```

程序

```JAVA

import org.apache.flink.api.common.functions.RichFilterFunction;
import org.apache.flink.api.common.functions.RichMapFunction;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;

public class DefaultChainTemplate {

    public static void main(String[] args) throws Exception {

        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
//        env.setParallelism(3);

        DataStream<Tuple2<String, Integer>> inputStream = env.addSource(new ChainSource());

        DataStream<Tuple2<String, Integer>> filter = inputStream.filter(
                new RichFilterFunction<Tuple2<String, Integer>>() {
                    @Override
                    public boolean filter(Tuple2<String, Integer> value) {
                        System.out.println("filter操作所属子任务名称:" + getRuntimeContext().getTaskNameWithSubtasks() + ",元素:" + value);
                        return true;
                    }
                });

        DataStream<Tuple2<String, Integer>> mapOne = filter.map(new RichMapFunction<Tuple2<String, Integer>, Tuple2<String, Integer>>() {
            @Override
            public Tuple2<String, Integer> map(Tuple2<String, Integer> value) {
                System.out.println("map-one操作所属子任务名称:" + getRuntimeContext().getTaskNameWithSubtasks() + ",元素:" + value);
                return value;
            }
        });

        DataStream<Tuple2<String, Integer>> mapTwo = mapOne.map(new RichMapFunction<Tuple2<String, Integer>, Tuple2<String, Integer>>() {
            @Override
            public Tuple2<String, Integer> map(Tuple2<String, Integer> value) {
                System.out.println("map-two操作所属子任务名称:" + getRuntimeContext().getTaskNameWithSubtasks() + ",元素:" + value);
                return value;
            }
        });
        mapTwo.print();

        env.execute("chain");
    }

}
```

## 开启新链接

![1654501479749](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1654501479749.jpg)

```JAVA

public class NewChainTemplate {

    public static void main(String[] args) throws Exception {

        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(3);

        DataStream<Tuple2<String, Integer>> inputStream = env.addSource(new ChainSource());

        DataStream<Tuple2<String, Integer>> filter = inputStream.filter(new RichFilterFunction<Tuple2<String, Integer>>() {
            @Override
            public boolean filter(Tuple2<String, Integer> value) {
                System.out.println("filter操作所属子任务名称:" + getRuntimeContext().getTaskNameWithSubtasks() + ",元素:" + value);
                return true;
            }
        });

        DataStream<Tuple2<String, Integer>> mapOne = filter.map(new RichMapFunction<Tuple2<String, Integer>, Tuple2<String, Integer>>() {
            @Override
            public Tuple2<String, Integer> map(Tuple2<String, Integer> value) {
                System.out.println("map-one操作所属子任务名称:" + getRuntimeContext().getTaskNameWithSubtasks() + ",元素:" + value);
                return value;
            }
        });


        mapOne = ((SingleOutputStreamOperator<Tuple2<String, Integer>>) mapOne).startNewChain();

        DataStream<Tuple2<String, Integer>> mapTwo = mapOne.map(new RichMapFunction<Tuple2<String, Integer>, Tuple2<String, Integer>>() {
            @Override
            public Tuple2<String, Integer> map(Tuple2<String, Integer> value) {
                System.out.println("map-two操作所属子任务名称:" + getRuntimeContext().getTaskNameWithSubtasks() + ",元素:" + value);
                return value;
            }
        });
        mapTwo.print();

        env.execute("chain");
    }

}
```

## 禁用链接

![1654502059959](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1654502059959.jpg)

```JAVA

public class DisableChainTemplate {

    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(3);
//        env.disableOperatorChaining();

        DataStream<Tuple2<String, Integer>> inputStream = env.addSource(new ChainSource());

        DataStream<Tuple2<String, Integer>> filter = inputStream.filter(new RichFilterFunction<Tuple2<String, Integer>>() {
            @Override
            public boolean filter(Tuple2<String, Integer> value) {
                System.out.println("filter操作所属子任务名称:" + getRuntimeContext().getTaskNameWithSubtasks() + ",元素:" + value);
                return true;
            }
        });

        DataStream<Tuple2<String, Integer>> mapOne = filter.map(new RichMapFunction<Tuple2<String, Integer>, Tuple2<String, Integer>>() {
            @Override
            public Tuple2<String, Integer> map(Tuple2<String, Integer> value) {
                System.out.println("map-one操作所属子任务名称:" + getRuntimeContext().getTaskNameWithSubtasks() + ",元素:" + value);
                return value;
            }
        });

        mapOne = ((SingleOutputStreamOperator<Tuple2<String,Integer>>) mapOne).disableChaining();

        DataStream<Tuple2<String, Integer>> mapTwo = mapOne.map(new RichMapFunction<Tuple2<String, Integer>, Tuple2<String, Integer>>() {
            @Override
            public Tuple2<String, Integer> map(Tuple2<String, Integer> value) {
                System.out.println("map-two操作所属子任务名称:" + getRuntimeContext().getTaskNameWithSubtasks() + ",元素:" + value);
                return value;
            }
        });
        mapTwo.print();

        env.execute("disableChaining");
    }

}
```

## 设置任务槽共享组

![1654503460107](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1654503460107.jpg)

# 5 物理分区

![1654504111985](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1654504111985.jpg)

## 数据源

````JAVA

public class PartitionSource extends RichSourceFunction<Trade> {

    private static final long serialVersionUID = 1L;

    @Override
    public void run(SourceContext<Trade> ctx) {
        List<Trade> list = new ArrayList<Trade>();
        list.add(new Trade("185XXX", 899, "2018"));
        list.add(new Trade("155XXX", 1111, "2019"));
        list.add(new Trade("155XXX", 1199, "2019"));
        list.add(new Trade("185XXX", 899, "2018"));
        list.add(new Trade("138XXX", 19, "2019"));
        list.add(new Trade("138XXX", 399, "2020"));

        for (int i = 0; i < list.size(); i++) {
            Trade trade = list.get(i);
            ctx.collect(trade);
        }
        String subtaskName = getRuntimeContext().getTaskNameWithSubtasks();
        System.out.println("source操作所属子任务名称:" + subtaskName);
    }

    @Override
    public void cancel() {
    }
}
````

## 自定义分区策略

![1654505216020](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1654505216020.jpg)

![1654505325140](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1654505325140.jpg)

---

自定义分区策略类

![1654505590671](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1654505590671.jpg)

```java
import org.apache.flink.api.common.functions.Partitioner;
public class MyPartitioner implements Partitioner<String> {
    @Override
    public int partition(String key, int numPartitions) {
        if (key.indexOf("185") >= 0) {
            return 0;
        } else if (key.indexOf("155") >= 0) {
            return 1;
        } else {
            return 2;
        }
    }
}
```

自定义分区实例

```java
public class CustomTemplate {

    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(3);

        final String flag = " 分区策略前子任务名称:";
        DataStream<Trade> inputStream = env.addSource(new PartitionSource());

        DataStream<Trade> mapOne = inputStream.map(new RichMapFunction<Trade, Trade>() {
            @Override
            public Trade map(Trade value) {
                RuntimeContext context = getRuntimeContext();
                String subtaskName = context.getTaskNameWithSubtasks();
                int subtaskIndexOf = context.getIndexOfThisSubtask();
                System.out.println("元素值:" + value + flag + subtaskName
                        + " ,子任务编号:" + subtaskIndexOf);
                return value;
            }
        });

        DataStream<Trade> mapTwo = mapOne.partitionCustom(new MyPartitioner(), "cardNum");


        DataStream<Trade> mapThree = mapTwo.map(new RichMapFunction<Trade, Trade>() {
            @Override
            public Trade map(Trade value) {
                RuntimeContext context = getRuntimeContext();
                String subtaskName = context.getTaskNameWithSubtasks();
                int subtaskIndexOf = context.getIndexOfThisSubtask();
                System.out.println("元素值:" + value + " 分区策略后子任务名称:" + subtaskName
                        + " ,子任务编号:" + subtaskIndexOf);
                return value;
            }
        });
        mapThree.print();
        env.execute("Physical partitioning");
    }

}
```

---

![1658196173505](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1658196173505.jpg)

同时需要修改CustomTemplate文件中的

```java
        DataStream<Trade> mapTwo = mapOne.partitionCustom(new MyTradePartitioner(), new KeySelector<Trade, Trade>() {
            @Override
            public Trade getKey(Trade trade) throws Exception {
                return trade;
            }
        });
```

## shuffle分区策略

随机函数，通过在数据流中调用shuffle方法使该数据流具备随机分区策略`DataStream<Trade> mapTwo = mapOne.shuffle()`

# 6 流处理的本地测试

## 集合支持的数据源与数据接收器

通过Flink提供的DataStreamUtils工具类可以将流处理结果收集到Java迭代器中

```java
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        DataStream<Long> streamSource = env.generateSequence(1, 5);

        DataStream<Long> mapStream = streamSource
                .map(new MapFunction<Long, Long>() {
                    @Override
                    public Long map(Long value) {
                        return value + 1;
                    }
                });
//        mapStream.print("输出结果");
        Iterator<Long> myoutput = DataStreamUtils.collect(mapStream);
        while (myoutput.hasNext()) {
            System.out.println(myoutput.next());
        }
```

# 7 分布式缓存

![1658213693634](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1658213693634.jpg)

## 注册分布式缓存文件

![1658213872615](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1658213872615.jpg)

## 访问分布式缓存文件

![1658214342095](https://raw.githubusercontent.com/liang636600/cloudImg/master/images/1658214342095.jpg)

```java
package org.example;

import org.apache.commons.lang3.StringUtils;
import org.apache.flink.api.common.cache.DistributedCache;
import org.apache.flink.api.common.functions.RichMapFunction;
import org.apache.flink.api.common.functions.RuntimeContext;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileReader;
import java.io.IOException;

public class DistributedCacheTemplate {

    public static void main(String[] args) throws Exception {

        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        String cacheUrl = "file:///home/driverliang/input.txt";
//        env.registerCachedFile("file:///home/intsmaze/flink/cache/local.txt", "localFile");
        env.registerCachedFile(cacheUrl, "localFile");

        DataStream<Long> input = env.generateSequence(1, 10);

        input.map(new MyMapper()).print();
        env.execute();
    }

    public static class MyMapper extends RichMapFunction<Long, String> {

        private String cacheStr;

        @Override
        public void open(Configuration config) {
            RuntimeContext runtimeContext = getRuntimeContext();
            DistributedCache distributedCache = runtimeContext.getDistributedCache();
            File myFile = distributedCache.getFile("localFile");
            cacheStr = readFile(myFile);
        }

        @Override
        public String map(Long value) throws Exception {
            Thread.sleep(10000);
            return StringUtils.join(value, "---", cacheStr);
        }


        public String readFile(File myFile) {
            BufferedReader reader = null;
            StringBuffer sbf = new StringBuffer();
            try {
                reader = new BufferedReader(new FileReader(myFile));
                String tempStr;
                while ((tempStr = reader.readLine()) != null) {
                    sbf.append(tempStr);
                }
                reader.close();
                return sbf.toString();
            } catch (IOException e) {
                e.printStackTrace();
            } finally {
                if (reader != null) {
                    try {
                        reader.close();
                    } catch (IOException e1) {
                        e1.printStackTrace();
                    }
                }
            }
            return sbf.toString();
        }
    }

}
```

# 8 将参数传递给函数

## 通过构造函数传递参数

```java
import org.apache.flink.api.common.functions.FilterFunction;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;

public class ConstructorTemplate {
    public static void main(String[] args) throws Exception {

        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(2);

        DataStream<Long> dataStream = env.generateSequence(1, 15);

        ParamBean paramBean = new ParamBean("intsmaze", 10);
        DataStream<Long> resultStream = dataStream
                .filter(new FilterConstructed(paramBean));

        resultStream.print("constructed stream is :");
        env.execute("ParamTemplate intsmaze");
    }

    private static class FilterConstructed implements FilterFunction<Long> {

        private final ParamBean paramBean;
        public FilterConstructed(ParamBean paramBean) {
            this.paramBean = paramBean;
        }

        @Override
        public boolean filter(Long value) {
            return value > paramBean.getFlag();
        }
    }
}
```

```JAVA
import java.io.Serializable;

public class ParamBean implements Serializable {

    private String name;

    private int flag;

    public ParamBean(String name, int flag) {
        this.name = name;
        this.flag = flag;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public int getFlag() {
        return flag;
    }

    public void setFlag(int flag) {
        this.flag = flag;
    }

    @Override
    public String toString() {
        return "ParamBean{" +
                "name='" + name + '\'' +
                ", flag=" + flag +
                '}';
    }
}
```

## 使用ExecutionConfig传递参数

### 设置自定义配置值

```java
import org.apache.flink.api.common.ExecutionConfig;
import org.apache.flink.api.common.functions.RichFilterFunction;
import org.apache.flink.api.java.utils.ParameterTool;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;

import java.io.InputStream;
public class ParamTemplate {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(2);

        Configuration conf = new Configuration();
        conf.setLong("limit", 16);
        ExecutionConfig executionConfig = env.getConfig();
        executionConfig.setGlobalJobParameters(conf);

        DataStream<Long> dataStream = env.generateSequence(1, 20);

        dataStream.filter(new FilterJobParameters())
                .print("JobParameters stream is :");

        env.execute("ParamTemplate intsmaze");
    }

    private static class FilterJobParameters extends RichFilterFunction<Long> {

        protected long limit;

        @Override
        public void open(Configuration parameters) {
            ExecutionConfig executionConfig = getRuntimeContext().getExecutionConfig();
            ExecutionConfig.GlobalJobParameters globalParams = executionConfig.getGlobalJobParameters();
            Configuration globConf = (Configuration) globalParams;
            limit = globConf.getLong("limit", 0);
        }

        @Override
        public boolean filter(Long value) {
            return value > limit;
        }
    }
}

```

## ParameterTool

### 读取来自properties文件中的数据

```java
ParameterTool parameter = ParameterTool.fromPropertiesFile(new FileInputStream("flink-param.properties"));
        String s = parameter.get("input", "DE");
```

